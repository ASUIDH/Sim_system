[param]

############################################
# training data dictory
############################################
# main work dir
root_dir = D:\\MD-data\\vt
# the name of one dataset, such as facebook, linkedin
dataset_name = vt
# the suffix of dataset, such as 10,100,1000
suffix = 10
# the relation name of data, such as classmate, family, school, work
class_name = mal
# the index of the dataset file
index = 1240

############################################
# paths for some prepared data
############################################
# the file path of words embeddings
wordsEmbeddings_path = %(root_dir)s/%(dataset_name)s/nodesFeatures
# the file which contains sub-paths
subpaths_file = %(root_dir)s/%(dataset_name)s/subpathsSaveFile

############################################
# experiment parameters - do not need to change frequently
############################################
# the max length for sub-paths
maxlen_subpaths = 1000
# the size of words vocabulary
wordsSize = 1000000
# Sequence longer than this get ignored 
maxlen = 1000
# use a batch for training. This is the size of this batch.(之前是32)
batch_size = 8
# if need shuffle for training
is_shuffle_for_batch = True
# the frequences for display
dispFreq = 1000
# the frequences for saving the parameters
saveFreq =2
# the path for saving parameters. It is generated by main_dir, dataset_name, suffix, class_name and index. It will be generated in the code.
saveto = 
# the top num to predict
top_num = 20

############################################
# experiment parameters - need to tune
############################################
# learning rate 之前是0.0001 我加了3倍，因为觉得优化的有点慢 这个lr好像是没用的参数，在梯度计算得函数中体现不出来
lrate = 0.0001
# dimension of words embeddings
word_dimension = 18
# the dimension of paths embeddings
dimension = 20
# the output way of lstm. There are three ways, "h" only uses the last output h as the output of lstm for one path; "mean-pooling" uses the mean-pooling of all hi as the output of lstm for one path; "max-pooling" uses the max-pooling of all hi as the output of lstm for one path.
h_output_method = max-pooling
# the parameter alpha for discount. The longer the subpath, the little will the weight be.
discount_alpha = 0.5
# the ways to combine several subpaths to one. "mean-pooling" means to combine all subpaths to one by mean-pooling; "max-pooling" means to combine all subpaths to one by max-pooling.
subpaths_pooling_method = max-pooling
# loss function, we use sigmoid
objective_function_method = sigmoid
# the parameter in loss function, beta
objective_function_param = 1
# the max epochs for training
max_epochs = 2
# decay for lstm_W
decay_lstm_W = 0.0003
# decay for lstm_U
decay_lstm_U = 0.0003
# decay for lstm_b
decay_lstm_b = 0.0003
# decay for w
decay_w = 0.0003


#test for ipe
sequences_file= %(root_dir)s/%(dataset_name)s/sequencesSaveFile
# repeat times
repeatTimes = 3
# the repear proportion
repeatProportion = 0.0
# the uplimit
